# -*- coding: utf-8 -*-
"""MNIST_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CHjizJzW0gRL4z88LxoQ1Fx3Z3ARAc_G
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import torchvision.transforms as transforms
from torch.utils.data.dataset import TensorDataset
import time

#define the NN model
class NN(nn.Module):
  def __init__(self, input_size, num_classes):
    super(NN, self).__init__()
    self.fc1 = nn.Linear(input_size, 50)
    self.fc2 = nn.Linear(50, num_classes)

  def forward(self, x):
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#Basic Test
model = NN(784, 10)
x = torch.randn(64,784)
model = model.to(device)
x = x.to(device)
start = time.process_time()
print(model(x).shape)
end = time.process_time()
print("time: ", end - start)

#parameters
in_channel = 1
num_classes = 10
learning_rate = 0.001
batch_size = 64
num_epochs = 100

#loading the data

X,y = fetch_openml("mnist_784", version = 1, return_X_y = True)

X = X.astype(np.float32)
y = np.int_(y)
X = X.reshape(X.shape[0], 784)
print(X.shape, y.shape)

X_tensor = torch.from_numpy(X)
y_tensor = torch.from_numpy(y)
y_tensor = y_tensor.type(torch.LongTensor)
X_train, X_test, y_train, y_test = train_test_split(X_tensor,y_tensor, test_size = (1/7), random_state = 42)

train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)
test_dataset = TensorDataset(X_test, y_test)
test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)

#initialise network
model = NN(784, num_classes)
loss_fun = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = learning_rate)

def chk_accuracy(loader, model):
    
  num_correct = 0
  num_samples = 0
  model.eval()
    
  with torch.no_grad():
    for x, y in loader:
      x = x.to(device = device)
      y = y.to(device = device)
      scores = model(x)
      predictions = scores.argmax(1)
      num_correct += sum((predictions == y))
      num_samples += predictions.size(0)
            
    return float(num_correct)/float(num_samples)

#Train the network
for epoch in range(num_epochs):
    model.train()
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    model = model.to(device = device)

    loss_train = 0
    start = time.process_time()
    for batch, (data, targets) in enumerate(train_loader):
      data = data.to(device = device)
      targets = targets.to(device= device)
        
      #Forward Prop
      scores = model(data)
      loss = loss_fun(scores, targets)
        
      #Back prop
      optimizer.zero_grad()
      loss.backward()
      loss_train += loss.item()

      #Optimizer
      optimizer.step()

    train_acc = chk_accuracy(train_loader, model)
    val_acc = chk_accuracy(test_loader, model)
    avg_loss = loss_train/(len(train_loader))
    end = time.process_time()

    print('Epoch ({}/{}),Training loss : {:.4f}, Time: {:.2f}, train_accuracy:{:.4f}, val_accuracy:{:.4f}'.format(epoch+1, num_epochs, avg_loss, end - start, train_acc, val_acc))